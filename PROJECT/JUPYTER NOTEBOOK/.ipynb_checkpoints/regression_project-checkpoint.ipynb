{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72ff6c66",
   "metadata": {},
   "source": [
    "# Regression - G3.2\n",
    "\n",
    "## 1. Dataset Analysis\n",
    "Load datasets via the scikit-learn library. The datasets can be downloaded using the fetch_openml function by indicating the name of the dataset as a parameter. In addition, organise the downloaded data in a pandas DataFrame, and display the first rows to gain an overview of the available variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddc962a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "from scipy.stats import normaltest\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load dataset using fetch_openml\n",
    "meta = fetch_openml(name='meta', version=1, parser='auto')\n",
    "california_housing = fetch_openml(name='california_housing', version=7, parser='auto')\n",
    "kin8nm = fetch_openml(name='kin8nm', version=1, parser='auto')\n",
    "chscase_census2 = fetch_openml(name='chscase_census2', version=1, parser='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5643c9e4",
   "metadata": {},
   "source": [
    "### 1.1 Meta\n",
    "Source Link: https://openml.org/search?type=data&status=active&id=566\n",
    "\n",
    "\n",
    "We now start by saving our dataset data to a support variable that we will later use to save the information needed for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d68c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_meta = meta.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af840e1",
   "metadata": {},
   "source": [
    "#### 1.1.A Find the dimensions of the dataset\n",
    "After loading the dataset, we begin by examining its size; the dataset consists of 528 samples and contains 22 features or variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a58acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\nDataset dimensions: \") \n",
    "data_meta.shape #(samples, n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e06417",
   "metadata": {},
   "source": [
    "#### 1.1.B Look at first one hundred few records of the dataset\n",
    "Printing the first one hundred rows gives us a first impression of the data, data types, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e92c2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\nFirst one hundred few records: \") \n",
    "data_meta.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14748e82",
   "metadata": {},
   "source": [
    "We chose to display a compact version of the first 100 rows to immediately notice possible relationships within the data. Also, we can immediately note the presence of some missing values appearing within the dataset, which will be duly dealt with in the second section of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f8d6e5",
   "metadata": {},
   "source": [
    "#### 1.1.C Look the main information that makes up the Dataset\n",
    "We can now print out the characteristics within the dataset that were requested by the analysis text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2487c726",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_meta = meta.target\n",
    "feature_names_meta = meta.feature_names\n",
    "description_meta = meta.DESCR\n",
    "\n",
    "# Now we can print data, target, labels and dataset information as needed\n",
    "print(\"\\n\\nData:\")\n",
    "print(data_meta)\n",
    "print(\"\\n\\nTarget:\")\n",
    "print(target_meta)\n",
    "print(\"\\n\\nFeatures name:\")\n",
    "print(feature_names_meta)\n",
    "print(\"\\n\\nDataset description:\")\n",
    "print(description_meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c30f4e",
   "metadata": {},
   "source": [
    "#### 1.1.D DataFrame creation with Pandas\n",
    "After performing all our initial checks within the Dataset, we can proceed to create the DataFrame using the Pandas library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d668ece8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta = pd.DataFrame(data_meta, columns=feature_names_meta)\n",
    "df_meta['target'] = target_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752be06b",
   "metadata": {},
   "source": [
    "### 1.2 California_Housing \n",
    "Source Link: https://openml.org/search?type=data&status=active&id=44977&sort=runs\n",
    "\n",
    "\n",
    "We now start by saving our dataset data to a support variable that we will later use to save the information needed for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb5c931",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ch = california_housing.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b33c8e5",
   "metadata": {},
   "source": [
    "#### 1.2.A Find the dimensions of the dataset\n",
    "After loading the dataset, we begin by examining its size; the dataset consists of 20640 samples and contains 8 features or variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d051ff1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\nDataset dimensions: \") \n",
    "data_ch.shape #(samples, n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d24dd7",
   "metadata": {},
   "source": [
    "#### 1.2.B Look at first few records of the dataset\n",
    "Printing the first rows gives us a first impression of the data, data types, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db70e781",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\nFirst few records: \") \n",
    "data_ch.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ff0315",
   "metadata": {},
   "source": [
    "In this particular case, with an initial analysis, we did not notice any unusual relationships between the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f93b59",
   "metadata": {},
   "source": [
    "#### 1.2.C Look the main information that makes up the Dataset\n",
    "We can now print out the characteristics within the dataset that were requested by the analysis text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b914f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_ch = california_housing.target\n",
    "feature_names_ch = california_housing.feature_names\n",
    "description_ch = california_housing.DESCR\n",
    "\n",
    "# Now we can print data, target, labels and dataset information as needed\n",
    "print(\"\\n\\nData:\")\n",
    "print(data_ch)\n",
    "print(\"\\n\\nTarget:\")\n",
    "print(target_ch)\n",
    "print(\"\\n\\nFeatures name:\")\n",
    "print(feature_names_ch)\n",
    "print(\"\\n\\nDataset description:\")\n",
    "print(description_ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea40fa3",
   "metadata": {},
   "source": [
    "#### 1.2.D DataFrame creation with Pandas\n",
    "After performing all our initial checks within the Dataset, we can proceed to create the DataFrame using the Pandas library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935c7940",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ch = pd.DataFrame(data_ch, columns=feature_names_ch)\n",
    "df_ch['target'] = target_ch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd1c359",
   "metadata": {},
   "source": [
    "### 1.3 Kin8nm\n",
    "Source Link: https://openml.org/search?type=data&id=189&sort=runs&status=active\n",
    "\n",
    "\n",
    "\n",
    "We now start by saving our dataset data to a support variable that we will later use to save the information needed for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6282458b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_kin8nm = kin8nm.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514a92e1",
   "metadata": {},
   "source": [
    "#### 1.3.A Find the dimensions of the dataset\n",
    "After loading the dataset, we begin by examining its size; the dataset consists of 8192 samples and contains 8 features or variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9258477d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\nDataset dimensions: \") \n",
    "data_kin8nm.shape #(samples, n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25892c3f",
   "metadata": {},
   "source": [
    "#### 1.3.B Look at first few records of the dataset\n",
    "Printing the first rows gives us a first impression of the data, data types, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd4d07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\nFirst few records: \") \n",
    "data_kin8nm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb47b3d",
   "metadata": {},
   "source": [
    "In this particular case, with an initial analysis, we did not notice any unusual relationships between the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851d007c",
   "metadata": {},
   "source": [
    "#### 1.3.C Look the main information that makes up the Dataset\n",
    "We can now print out the characteristics within the dataset that were requested by the analysis text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b070bc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_kin8nm = kin8nm.target\n",
    "feature_names_kin8nm = kin8nm.feature_names\n",
    "description_kin8nm = kin8nm.DESCR\n",
    "\n",
    "# Now we can print data, target, labels and dataset information as needed\n",
    "print(\"\\n\\nData:\")\n",
    "print(data_kin8nm)\n",
    "print(\"\\n\\nTarget:\")\n",
    "print(target_kin8nm)\n",
    "print(\"\\n\\nFeatures name:\")\n",
    "print(feature_names_kin8nm)\n",
    "print(\"\\n\\nDataset description:\")\n",
    "print(description_kin8nm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e3bd59",
   "metadata": {},
   "source": [
    "#### 1.3.D DataFrame creation with Pandas\n",
    "After performing all our initial checks within the Dataset, we can proceed to create the DataFrame using the Pandas library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d8b397",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kin8nm = pd.DataFrame(data_kin8nm, columns=feature_names_kin8nm)\n",
    "df_kin8nm['target'] = target_kin8nm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fedda3",
   "metadata": {},
   "source": [
    "### 1.4 Chscase_Census2\n",
    "Source Link: https://openml.org/search?type=data&id=673&sort=runs&status=active\n",
    "\n",
    "\n",
    "We now start by saving our dataset data to a support variable that we will later use to save the information needed for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e22f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_chcc2 = chscase_census2.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a79218",
   "metadata": {},
   "source": [
    "#### 1.4.A Find the dimensions of the dataset\n",
    "After loading the dataset, we begin by examining its size; the dataset consists of 400 samples and contains 7 features or variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01614316",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\nDataset dimensions: \") \n",
    "data_chcc2.shape #(samples, n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b9bfb1",
   "metadata": {},
   "source": [
    "#### 1.4.B Look at first few records of the dataset\n",
    "Printing the first rows gives us a first impression of the data, data types, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cbd788",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\nFirst few records: \") \n",
    "data_chcc2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab4ce01",
   "metadata": {},
   "source": [
    "In this particular case, with an initial analysis, we did not notice any unusual relationships between the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4404fb4",
   "metadata": {},
   "source": [
    "#### 1.4.C Look the main information that makes up the Dataset\n",
    "We can now print out the characteristics within the dataset that were requested by the analysis text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b34aa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_chcc2 = chscase_census2.target\n",
    "feature_names_chcc2 = chscase_census2.feature_names\n",
    "description_chcc2 = chscase_census2.DESCR\n",
    "\n",
    "# Now we can print data, target, labels and dataset information as needed\n",
    "print(\"\\n\\nData:\")\n",
    "print(data_chcc2)\n",
    "print(\"\\n\\nTarget:\")\n",
    "print(target_chcc2)\n",
    "print(\"\\n\\nFeatures name:\")\n",
    "print(feature_names_chcc2)\n",
    "print(\"\\n\\nDataset description:\")\n",
    "print(description_chcc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362e3342",
   "metadata": {},
   "source": [
    "#### 1.4.D DataFrame creation with Pandas\n",
    "After performing all our initial checks within the Dataset, we can proceed to create the DataFrame using the Pandas library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53367120",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chcc2 = pd.DataFrame(data_chcc2, columns=feature_names_chcc2)\n",
    "df_chcc2['target'] = target_chcc2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01308c7",
   "metadata": {},
   "source": [
    "## 2. Preprocessing\n",
    "Now, we will proceed with the necessary preprocessing activities, focusing on the elimination of nominal features and the handling of missing values. Finally, we will perform normalisation or standardisation of variables to ensure that the data are ready for the next steps of predictive analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7d0dc7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Features Variances\n",
    "The second function was created to represent the variance between all features within the dataset in a more structured manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e844fccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_feature_variances(data):\n",
    "    variances = data.var()\n",
    "\n",
    "    feature_variances = pd.DataFrame({\n",
    "        'Feature': variances.index,\n",
    "        'Variance': variances.values\n",
    "    })\n",
    "\n",
    "    return feature_variances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04f16aa",
   "metadata": {},
   "source": [
    "#### Density plots for features distribution\n",
    "Can we used this function to check the correct features distribution inside the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247e510d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_numeric_distributions(dataframe, numeric_features):\n",
    "    num_features_count = len(numeric_features.columns)\n",
    "    rows = (num_features_count // 2) + (num_features_count % 2 > 0)\n",
    "    fig, ax = plt.subplots(rows, 2, figsize=(15, 2 * rows))\n",
    "    fig.suptitle('Features Distribution Plot\\n\\n', fontsize=16)\n",
    "\n",
    "    row, col = 0, 0\n",
    "    for n, feature in enumerate(numeric_features):\n",
    "        if (n % 2 == 0) and (n > 0):\n",
    "            row += 1\n",
    "            col = 0\n",
    "\n",
    "        dataframe[feature].plot(kind=\"kde\", ax=ax[row, col])\n",
    "        ax[row, col].set_title(feature)\n",
    "        col += 1\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73cc2bf",
   "metadata": {},
   "source": [
    "### 2.1 Meta\n",
    "\n",
    "\n",
    "#### 2.1.A Check for nominal features\n",
    "In this step, we will try to better understand what types of data are present within the dataset. In the case of anomalous types, adopt the choices that will be discussed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0033e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\nTypes data:\")\n",
    "df_meta.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dce1d43",
   "metadata": {},
   "source": [
    "We can immediately see that there are two nominal (or categorical) features within the dataset. In this case, the text advises us to delete them should there be any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f59043",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Delete nominal features (if any)\n",
    "df_meta = df_meta.select_dtypes(exclude=['category'])\n",
    "\n",
    "print(\"\\n\\nTypes data after removal:\")\n",
    "df_meta.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db826d0b",
   "metadata": {},
   "source": [
    "#### 2.1.B Check for any missing data\n",
    "We can now proceed to search for any missing data within the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f380ce2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0428f85c",
   "metadata": {},
   "source": [
    "We can see that there are considerable amounts of null values for three different features. Therefore, we will now proceed with the interpolation of the missing instances, as about half of the total instances are absent. In fact, it is worth remembering that in the case of missing data, it is not recommended to delete them, but rather to interpolate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61512f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta = df_meta.interpolate(method='cubicspline', limit_direction='both', axis=0)\n",
    "\n",
    "df_meta.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b0e7fa",
   "metadata": {},
   "source": [
    "Now there are no longer any missing data within the dataset.\n",
    "\n",
    "\n",
    "##### 2.1.C Standardisation of values\n",
    "In this step, what we are going to do is to standardise the values within the dataset, to try to fit them within a single range or scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e81cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We standardising the values within the Dataset\n",
    "numeric_cols_meta = df_meta.select_dtypes(include=['float64', 'int64']).columns\n",
    "df_meta[numeric_cols_meta] = StandardScaler().fit_transform(df_meta[numeric_cols_meta])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c131e9ba",
   "metadata": {},
   "source": [
    "##### 2.1.D Variance control between features\n",
    "Having previously carried out the standardisation, the variance of each feature is examined. Also, in this step, we try to understand whether it is necessary to apply further Feature Extraction techniques in order to make the dataset \"lighter\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c42a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = calculate_feature_variances(df_meta)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803755a8",
   "metadata": {},
   "source": [
    "In this case it is approximately equal to 1, which suggests that the features have been homogenised in terms of scale. In addition, the fact that the variance is uniform indicates that all features contribute similarly to the overall variance of the dataset and therefore no features need to be eliminated on the basis of variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e447ba",
   "metadata": {},
   "source": [
    "##### 2.1.E Verification of operations\n",
    "In the following we will check whether the standardisation yielded the desired results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c90a94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f87162f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_numeric_distributions(df_meta, df_meta[numeric_cols_meta])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c377817",
   "metadata": {},
   "source": [
    "In this case, we can say with certainty that the missing values were interpolated correctly and the standardisation was successful. We can certainly see by eye that the features shown in the graphs above do not follow a Gaussian distribution. But this does not matter much to us, because the models we will use later turn out to be robust to the normality of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a419addd",
   "metadata": {},
   "source": [
    "### 2.2 California_Housing\n",
    "\n",
    "\n",
    "#### 2.2.A Check for nominal features\n",
    "In this step, we will try to better understand what types of data are present within the dataset. In the case of anomalous types, adopt the choices that will be discussed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36b0778",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\nTypes data:\")\n",
    "df_ch.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7c2b7d",
   "metadata": {},
   "source": [
    "In this case, nominal features do not appear, so we can proceed to check for missing data without eliminating these features.\n",
    "\n",
    "\n",
    "#### 2.2.B Check for any missing data\n",
    "We can now proceed to search for any missing data within the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb6a9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ch.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066c0319",
   "metadata": {},
   "source": [
    "Also in this step, we had no problems as there is no missing data within the dataset. Therefore, we can proceed with the next step, which concerns standardisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c204f1",
   "metadata": {},
   "source": [
    "##### 2.2.C Standardisation of values\n",
    "In this step, what we are going to do is to standardise the values within the dataset, to try to fit them within a single range or scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef9bea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols_ch = df_ch.select_dtypes(include=['float64', 'int64']).columns\n",
    "df_ch[numeric_cols_ch] = StandardScaler().fit_transform(df_ch[numeric_cols_ch])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ea7ea4",
   "metadata": {},
   "source": [
    "##### 2.2.D Variance control between features\n",
    "Having previously carried out the standardisation, the variance of each feature is examined. Also, in this step, we try to understand whether it is necessary to apply further Feature Extraction techniques in order to make the dataset \"lighter\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a79181",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = calculate_feature_variances(df_ch)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f71cec4",
   "metadata": {},
   "source": [
    "In this case it is approximately equal to 1, which suggests that the features have been homogenised in terms of scale. In addition, the fact that the variance is uniform indicates that all features contribute similarly to the overall variance of the dataset and therefore no features need to be eliminated on the basis of variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fea7266",
   "metadata": {},
   "source": [
    "##### 2.2.E Verification of operations\n",
    "In the following we will check whether the standardisation yielded the desired results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34292dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ch.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d441824e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_numeric_distributions(df_ch, df_ch[numeric_cols_ch])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444f445b",
   "metadata": {},
   "source": [
    "In this case, we can say with certainty that the missing values were interpolated correctly and the standardisation was successful. We can certainly see by eye that the features shown in the graphs above do not follow a Gaussian distribution. But this does not matter much to us, because the models we will use later turn out to be robust to the normality of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33942a33",
   "metadata": {},
   "source": [
    "### 2.3 Kin8nm\n",
    "\n",
    "\n",
    "#### 2.3.A Check for nominal features\n",
    "In this step, we will try to better understand what types of data are present within the dataset. In the case of anomalous types, adopt the choices that will be discussed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbad11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\nTypes data:\")\n",
    "df_kin8nm.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2958fbd",
   "metadata": {},
   "source": [
    "In this case, nominal features do not appear, so we can proceed to check for missing data without eliminating these features.\n",
    "\n",
    "\n",
    "#### 2.3.B Check for any missing data\n",
    "We can now proceed to search for any missing data within the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15356518",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kin8nm.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46310dd7",
   "metadata": {},
   "source": [
    "Also in this step, we had no problems as there is no missing data within the dataset. Therefore, we can proceed with the next step, which concerns standardisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfee2750",
   "metadata": {},
   "source": [
    "##### 2.3.C Standardisation of values\n",
    "In this step, what we are going to do is to standardise the values within the dataset, to try to fit them within a single range or scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d89bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We standardising the values within the Dataset\n",
    "numeric_cols_kin8nm = df_kin8nm.select_dtypes(include=['float64', 'int64']).columns\n",
    "df_kin8nm[numeric_cols_kin8nm] = StandardScaler().fit_transform(df_kin8nm[numeric_cols_kin8nm])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce6d268",
   "metadata": {},
   "source": [
    "##### 2.3.D Variance control between features\n",
    "Having previously carried out the standardisation, the variance of each feature is examined. Also, in this step, we try to understand whether it is necessary to apply further Feature Extraction techniques in order to make the dataset \"lighter\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10d4e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = calculate_feature_variances(df_kin8nm)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b87ea06",
   "metadata": {},
   "source": [
    "In this case it is approximately equal to 1, which suggests that the features have been homogenised in terms of scale. In addition, the fact that the variance is uniform indicates that all features contribute similarly to the overall variance of the dataset and therefore no features need to be eliminated on the basis of variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411d579e",
   "metadata": {},
   "source": [
    "##### 2.3.E Verification of operations\n",
    "In the following we will check whether the standardisation yielded the desired results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c40ac24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kin8nm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41993a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_numeric_distributions(df_kin8nm, df_kin8nm[numeric_cols_kin8nm])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4aefa2a",
   "metadata": {},
   "source": [
    "In this case, we can say with certainty that the missing values were interpolated correctly and the standardisation was successful. We can certainly see by eye that the features shown in the graphs above do not follow a Gaussian distribution. But this does not matter much to us, because the models we will use later turn out to be robust to the normality of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8e9d5e",
   "metadata": {},
   "source": [
    "### 2.4 Chscase_Census2\n",
    "\n",
    "\n",
    "#### 2.4.A Check for nominal features\n",
    "In this step, we will try to better understand what types of data are present within the dataset. In the case of anomalous types, adopt the choices that will be discussed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2561e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\nTypes data:\")\n",
    "df_chcc2.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b425b2",
   "metadata": {},
   "source": [
    "In this case, nominal features do not appear, so we can proceed to check for missing data without eliminating these features.\n",
    "\n",
    "\n",
    "#### 2.4.B Check for any missing data\n",
    "We can now proceed to search for any missing data within the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc6d1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chcc2.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9995b2b7",
   "metadata": {},
   "source": [
    "Also in this step, we had no problems as there is no missing data within the dataset. Therefore, we can proceed with the next step, which concerns standardisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed3d381",
   "metadata": {},
   "source": [
    "##### 2.4.C Standardisation of values\n",
    "In this step, what we are going to do is to standardise the values within the dataset, to try to fit them within a single range or scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990867f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We standardising the values within the Dataset\n",
    "numeric_cols_chcc2 = df_chcc2.select_dtypes(include=['float64', 'int64']).columns\n",
    "df_chcc2[numeric_cols_chcc2] = StandardScaler().fit_transform(df_chcc2[numeric_cols_chcc2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02720c10",
   "metadata": {},
   "source": [
    "##### 2.4.D Variance control between features\n",
    "Having previously carried out the standardisation, the variance of each feature is examined. Also, in this step, we try to understand whether it is necessary to apply further Feature Extraction techniques in order to make the dataset \"lighter\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4cbd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = calculate_feature_variances(df_chcc2)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18309582",
   "metadata": {},
   "source": [
    "In this case it is approximately equal to 1, which suggests that the features have been homogenised in terms of scale. In addition, the fact that the variance is uniform indicates that all features contribute similarly to the overall variance of the dataset and therefore no features need to be eliminated on the basis of variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66dfad1",
   "metadata": {},
   "source": [
    "##### 2.4.E Verification of operations\n",
    "In the following we will check whether the standardisation yielded the desired results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07292c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chcc2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686633aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_numeric_distributions(df_chcc2, df_chcc2[numeric_cols_chcc2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13e2a15",
   "metadata": {},
   "source": [
    "In this case, we can say with certainty that the missing values were interpolated correctly and that the standardisation was successful. We can certainly see by eye that the features shown in the graphs above do not follow a Gaussian distribution. The only feature that seems to follow a Gaussian distribution is the target variable. But this does not matter much to us, because the models we will use later turn out to be robust to the normality of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16886bfe",
   "metadata": {},
   "source": [
    "## 3. Regression\n",
    "First of all, let us start by defining some main functions that are fundamental to the third step:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2b06d9",
   "metadata": {},
   "source": [
    "#### SMAPE\n",
    "In this case, we implemented a special function, which takes two arrays, one for real values and one for predicted values, and returns the SMAPE according to the formula given in the course slides. The formula we used to implement this metric is the one we saw during the course, and thus:\n",
    "<center>$SMAPE = \\frac{1}{N} \\sum_{i=1}^N \\frac{|y_i - \\hat{y_i}|}{|y_i| + |\\hat{y_i}|}$</center>\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad5c888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape(y_true, y_pred):\n",
    "    return np.mean(np.abs(y_pred - y_true) / (np.abs(y_pred) + np.abs(y_true)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ab01fd",
   "metadata": {},
   "source": [
    "#### EVALUATION OF MODELS\n",
    "In this case, we considered it necessary and indispensable for the cleanliness of the code to create two functions that can be called several times within the code to calculate the results obtained by applying the different regression models required. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0c7fb6",
   "metadata": {},
   "source": [
    "The first function is to create a dataframe containing the metric values for each model applied during the regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111c5578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_dataframe(model_names, mae_scores, rmse_scores, mape_scores, smape_scores):\n",
    "    \n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Model': model_names,\n",
    "        'MAE': mae_scores,\n",
    "        'RMSE': rmse_scores,\n",
    "        'MAPE': mape_scores,\n",
    "        'SMAPE': smape_scores\n",
    "    })\n",
    "\n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f568e12d",
   "metadata": {},
   "source": [
    "Also, the second function is the very core of the metrics calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4388d4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name, common_exponent = -2):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "    smape_score = smape(y_test, y_pred)\n",
    "\n",
    "    return y_pred, mae, rmse, mape, smape_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58436c8f",
   "metadata": {},
   "source": [
    "#### CHART PLOT\n",
    "Again, we found it necessary to create two function that plots all graphs for a single dataset to keep the code clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d874b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter_plots(model_names, y_true, y_pred_scores):\n",
    "    num_models = len(model_names)\n",
    "    num_rows = 2\n",
    "    num_cols = 3\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 8))\n",
    "    fig.suptitle('\\n\\nScatter Plots of Model Predictions vs. True Values', fontsize=16)\n",
    "\n",
    "    for i, (model_name, y_pred) in enumerate(zip(model_names, y_pred_scores)):\n",
    "        if i == num_models - 1:\n",
    "            col = 2\n",
    "        else:\n",
    "            col = i % num_cols\n",
    "\n",
    "        row = i // num_cols\n",
    "\n",
    "        ax = plt.subplot2grid((num_rows, num_cols), (row, col))\n",
    "\n",
    "        ax.scatter(y_true, y_pred, alpha=0.7, label=model_name)\n",
    "        ax.plot([min(y_true), max(y_true)], [min(y_true), max(y_true)], color='red', linestyle='--', linewidth=2)\n",
    "        ax.set_title(model_name)\n",
    "        ax.set_xlabel('True Values')\n",
    "        ax.set_ylabel('Predicted Values')\n",
    "        ax.legend()\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f904db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(model_names, mae_scores, rmse_scores, mape_scores, smape_scores):\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('\\n\\nChart Plots of Evaluation Metrics Results', fontsize=16)\n",
    "\n",
    "    # MAE\n",
    "    axs[0, 0].bar(model_names, mae_scores, color='blue')\n",
    "    axs[0, 0].set_title('MAE')\n",
    "    axs[0, 0].set_ylabel('Mean Absolute Error')\n",
    "\n",
    "    # RMSE\n",
    "    axs[0, 1].bar(model_names, rmse_scores, color='orange')\n",
    "    axs[0, 1].set_title('RMSE')\n",
    "    axs[0, 1].set_ylabel('Root Mean Square Error')\n",
    "\n",
    "    # MAPE\n",
    "    axs[1, 0].bar(model_names, mape_scores, color='green')\n",
    "    axs[1, 0].set_title('MAPE')\n",
    "    axs[1, 0].set_ylabel('Mean Absolute Percentage Error')\n",
    "\n",
    "    # SMAPE\n",
    "    axs[1, 1].bar(model_names, smape_scores, color='red')\n",
    "    axs[1, 1].set_title('SMAPE')\n",
    "    axs[1, 1].set_ylabel('Symmetric Mean Absolute Percentage Error')\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.5)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dab7833",
   "metadata": {},
   "source": [
    "### 3.1 Meta\n",
    "In this regression phase, we divide the dataset into training and test sets. Next, to train and evaluate the proposed regression models. And finally, we will report the results of this regression through the specified metrics and their corresponding graphs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0520af",
   "metadata": {},
   "source": [
    "##### 3.1.A Divide dataset into Training and Test sets\n",
    "In this first step, we will divide the dataset into the previously specified datasets. To do this, we will create vectors where we will save the results of the various metrics, save the features of the dataframe and only then perform the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386c1cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_meta.drop(columns=['target'])\n",
    "target = df_meta['target']\n",
    "\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "mape_scores = []\n",
    "smape_scores = []\n",
    "y_pred_scores = []\n",
    "\n",
    "# Division of the DataFrame into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd1d887",
   "metadata": {},
   "source": [
    "#### 3.1.B Initialisation of models\n",
    "In this phase, the regression models, such as Linear Regression, Support Vector Machine, Decision Trees, Random Forest, and Gradient Boosting, are instantiated and prepared in order to be ready for the next training and evaluation phase on the dataset data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a86bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_reg_model = LinearRegression()\n",
    "svr_model = SVR()\n",
    "decision_tree_model = DecisionTreeRegressor()\n",
    "random_forest_model = RandomForestRegressor()\n",
    "gradient_boosting_model = GradientBoostingRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c0c0ca",
   "metadata": {},
   "source": [
    "The following approach makes it easier to manage and recall models during the training, testing and evaluation process, as model names are consistently associated with model objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ab97b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [linear_reg_model, svr_model, decision_tree_model, random_forest_model, gradient_boosting_model]\n",
    "model_names = ['Linear Regression', 'SVR', 'Decision Tree', 'Random Forest', 'Gradient Boosting']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889ac89f",
   "metadata": {},
   "source": [
    "##### 3.1.C Evaluation of models\n",
    "In this step, several regression models are evaluated using the META dataset. For each model, evaluation metrics such as MAE, RMSE, MAPE and SMAPE are calculated and recorded, and model predictions are saved for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc8d7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of models for the META Dataset\n",
    "for model, model_name in zip(models, model_names):\n",
    "    y_pred, mae, rmse, mape, smape_score = evaluate_model(model, X_train, X_test, y_train, y_test, model_name)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    mape_scores.append(mape)\n",
    "    smape_scores.append(smape_score)\n",
    "    y_pred_scores.append(y_pred)\n",
    "    \n",
    "models_df = metrics_dataframe(model_names, mae_scores, rmse_scores, mape_scores, smape_scores)\n",
    "\n",
    "models_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df000fb7",
   "metadata": {},
   "source": [
    "#### 3.1.D Graphical representation of results\n",
    "In this step, we are generating graphs to visualise the performance of the regression models. With the first function, we are comparing predicted values with actual values, while with the second function, we are using barplots to obtain a quick visualisation and insight into the metrics for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f91b214",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter_plots(model_names, y_test, y_pred_scores)\n",
    "plot_results(model_names, mae_scores, rmse_scores, mape_scores, smape_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d0617e",
   "metadata": {},
   "source": [
    "### 3.2 California_Housing\n",
    "In this regression phase, we divide the dataset into training and test sets. Next, to train and evaluate the proposed regression models. And finally, we will report the results of this regression through the specified metrics and their corresponding graphs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779ac909",
   "metadata": {},
   "source": [
    "##### 3.2.A Divide dataset into Training and Test sets\n",
    "In this first step, we will divide the dataset into the previously specified datasets. To do this, we will create vectors where we will save the results of the various metrics, save the features of the dataframe and only then perform the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4a1946",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_ch.drop(columns=['target'])\n",
    "target = np.log1p(df_ch['target'] - df_ch['target'].min() + 1)\n",
    "\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "mape_scores = []\n",
    "smape_scores = []\n",
    "y_pred_scores = []\n",
    "\n",
    "# Division of the DataFrame into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7658f898",
   "metadata": {},
   "source": [
    "As suggested by the description of the dataset, on OpenML, we decided to apply a logarithmic transformation to the target column. In addition, we also paid attention to possible negative or zero values; all this by adding one and subtracting the minimum value of the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfd13d2",
   "metadata": {},
   "source": [
    "#### 3.2.B Initialisation of models\n",
    "In this phase, the regression models, such as Linear Regression, Support Vector Machine, Decision Trees, Random Forest, and Gradient Boosting, are instantiated and prepared in order to be ready for the next training and evaluation phase on the dataset data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7360454c",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_reg_model = LinearRegression()\n",
    "svr_model = SVR()\n",
    "decision_tree_model = DecisionTreeRegressor()\n",
    "random_forest_model = RandomForestRegressor()\n",
    "gradient_boosting_model = GradientBoostingRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4265eca2",
   "metadata": {},
   "source": [
    "The following approach makes it easier to manage and recall models during the training, testing and evaluation process, as model names are consistently associated with model objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993370d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [linear_reg_model, svr_model, decision_tree_model, random_forest_model, gradient_boosting_model]\n",
    "model_names = ['Linear Regression', 'SVR', 'Decision Tree', 'Random Forest', 'Gradient Boosting']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244a6960",
   "metadata": {},
   "source": [
    "##### 3.2.C Evaluation of models\n",
    "In this step, several regression models are evaluated using the CALIFORNIA_HOUSING dataset. For each model, evaluation metrics such as MAE, RMSE, MAPE and SMAPE are calculated and recorded, and model predictions are saved for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7da107d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of models for the CALIFORNIA_HOUSING Dataset\n",
    "for model, model_name in zip(models, model_names):\n",
    "    y_pred, mae, rmse, mape, smape_score = evaluate_model(model, X_train, X_test, y_train, y_test, model_name)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    mape_scores.append(mape)\n",
    "    smape_scores.append(smape_score)\n",
    "    y_pred_scores.append(y_pred)\n",
    "    \n",
    "models_df = metrics_dataframe(model_names, mae_scores, rmse_scores, mape_scores, smape_scores)\n",
    "\n",
    "models_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b3f917",
   "metadata": {},
   "source": [
    "#### 3.2.D Graphical representation of results\n",
    "In this step, we are generating graphs to visualise the performance of the regression models. With the first function, we are comparing predicted values with actual values, while with the second function, we are using barplots to obtain a quick visualisation and insight into the metrics for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9b05a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter_plots(model_names, y_test, y_pred_scores)\n",
    "plot_results(model_names, mae_scores, rmse_scores, mape_scores, smape_scores) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26742ca9",
   "metadata": {},
   "source": [
    "### 3.3 Kin8nm\n",
    "In this regression phase, we divide the dataset into training and test sets. Next, to train and evaluate the proposed regression models. And finally, we will report the results of this regression through the specified metrics and their corresponding graphs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0736dd33",
   "metadata": {},
   "source": [
    "##### 3.3.A Divide dataset into Training and Test sets\n",
    "In this first step, we will divide the dataset into the previously specified datasets. To do this, we will create vectors where we will save the results of the various metrics, save the features of the dataframe and only then perform the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446451dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_kin8nm.drop(columns=['target'])\n",
    "target = df_kin8nm['target']\n",
    "\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "mape_scores = []\n",
    "smape_scores = []\n",
    "y_pred_scores = []\n",
    "\n",
    "# Division of the DataFrame into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c273989c",
   "metadata": {},
   "source": [
    "#### 3.3.B Initialisation of models\n",
    "In this phase, the regression models, such as Linear Regression, Support Vector Machine, Decision Trees, Random Forest, and Gradient Boosting, are instantiated and prepared in order to be ready for the next training and evaluation phase on the dataset data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dae836d",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_reg_model = LinearRegression()\n",
    "svr_model = SVR()\n",
    "decision_tree_model = DecisionTreeRegressor()\n",
    "random_forest_model = RandomForestRegressor()\n",
    "gradient_boosting_model = GradientBoostingRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14837b72",
   "metadata": {},
   "source": [
    "The following approach makes it easier to manage and recall models during the training, testing and evaluation process, as model names are consistently associated with model objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c039ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [linear_reg_model, svr_model, decision_tree_model, random_forest_model, gradient_boosting_model]\n",
    "model_names = ['Linear Regression', 'SVR', 'Decision Tree', 'Random Forest', 'Gradient Boosting']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33bbb9e",
   "metadata": {},
   "source": [
    "##### 3.3.C Evaluation of models\n",
    "In this step, several regression models are evaluated using the KIN8NM dataset. For each model, evaluation metrics such as MAE, RMSE, MAPE and SMAPE are calculated and recorded, and model predictions are saved for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc515eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of models for the KIN8NM Dataset\n",
    "for model, model_name in zip(models, model_names):\n",
    "    y_pred, mae, rmse, mape, smape_score = evaluate_model(model, X_train, X_test, y_train, y_test, model_name)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    mape_scores.append(mape)\n",
    "    smape_scores.append(smape_score)\n",
    "    y_pred_scores.append(y_pred)\n",
    "    \n",
    "models_df = metrics_dataframe(model_names, mae_scores, rmse_scores, mape_scores, smape_scores)\n",
    "\n",
    "models_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4264c94c",
   "metadata": {},
   "source": [
    "#### 3.3.D Graphical representation of results\n",
    "In this step, we are generating graphs to visualise the performance of the regression models. With the first function, we are comparing predicted values with actual values, while with the second function, we are using barplots to obtain a quick visualisation and insight into the metrics for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1456f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter_plots(model_names, y_test, y_pred_scores)\n",
    "plot_results(model_names, mae_scores, rmse_scores, mape_scores, smape_scores) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab46671",
   "metadata": {},
   "source": [
    "### 3.4 Cshcase_Census2\n",
    "In this regression phase, we divide the dataset into training and test sets. Next, to train and evaluate the proposed regression models. And finally, we will report the results of this regression through the specified metrics and their corresponding graphs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4730e813",
   "metadata": {},
   "source": [
    "##### 3.4.A Divide dataset into Training and Test sets\n",
    "In this first step, we will divide the dataset into the previously specified datasets. To do this, we will create vectors where we will save the results of the various metrics, save the features of the dataframe and only then perform the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f6a18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_chcc2.drop(columns=['target'])\n",
    "target = df_chcc2['target']\n",
    "\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "mape_scores = []\n",
    "smape_scores = []\n",
    "y_pred_scores = []\n",
    "\n",
    "# Division of the DataFrame into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336c262f",
   "metadata": {},
   "source": [
    "#### 3.4.B Initialisation of models\n",
    "In this phase, the regression models, such as Linear Regression, Support Vector Machine, Decision Trees, Random Forest, and Gradient Boosting, are instantiated and prepared in order to be ready for the next training and evaluation phase on the dataset data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e735ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_reg_model = LinearRegression()\n",
    "svr_model = SVR()\n",
    "decision_tree_model = DecisionTreeRegressor()\n",
    "random_forest_model = RandomForestRegressor()\n",
    "gradient_boosting_model = GradientBoostingRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9663ed3",
   "metadata": {},
   "source": [
    "The following approach makes it easier to manage and recall models during the training, testing and evaluation process, as model names are consistently associated with model objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6b0fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [linear_reg_model, svr_model, decision_tree_model, random_forest_model, gradient_boosting_model]\n",
    "model_names = ['Linear Regression', 'SVR', 'Decision Tree', 'Random Forest', 'Gradient Boosting']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ccce54",
   "metadata": {},
   "source": [
    "##### 3.4.C Evaluation of models\n",
    "In this step, several regression models are evaluated using the CSHCASE_CENSUS2 dataset. For each model, evaluation metrics such as MAE, RMSE, MAPE and SMAPE are calculated and recorded, and model predictions are saved for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed7f22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of models for the CSHCASE_CENSUS2 Dataset\n",
    "for model, model_name in zip(models, model_names):\n",
    "    y_pred, mae, rmse, mape, smape_score = evaluate_model(model, X_train, X_test, y_train, y_test, model_name)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    mape_scores.append(mape)\n",
    "    smape_scores.append(smape_score)\n",
    "    y_pred_scores.append(y_pred)\n",
    "    \n",
    "models_df = metrics_dataframe(model_names, mae_scores, rmse_scores, mape_scores, smape_scores)\n",
    "\n",
    "models_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38257717",
   "metadata": {},
   "source": [
    "#### 3.3.D Graphical representation of results\n",
    "In this step, we are generating graphs to visualise the performance of the regression models. With the first function, we are comparing predicted values with actual values, while with the second function, we are using barplots to obtain a quick visualisation and insight into the metrics for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acfd208",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter_plots(model_names, y_test, y_pred_scores)\n",
    "plot_results(model_names, mae_scores, rmse_scores, mape_scores, smape_scores) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
